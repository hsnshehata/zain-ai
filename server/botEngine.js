const OpenAI = require('openai');
const mongoose = require('mongoose');
const axios = require('axios');
const FormData = require('form-data');
const Bot = require('./models/Bot');

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const conversationSchema = new mongoose.Schema({
  botId: { type: mongoose.Schema.Types.ObjectId, ref: 'Bot', required: true },
  userId: { type: String, required: true },
  messages: [
    {
      role: { type: String, enum: ['user', 'assistant'], required: true },
      content: { type: String, required: true },
      timestamp: { type: Date, default: Date.now },
    },
  ],
});

const Conversation = mongoose.model('Conversation', conversationSchema);

const Rule = require('./models/Rule');

async function transcribeAudio(audioUrl) {
  const body = new FormData();
  body.append('file', audioUrl);
  body.append('language', 'arabic');
  body.append('response_format', 'json');
  try {
    console.log(
      'LemonFox API Key: ' +
        (process.env.LEMONFOX_API_KEY ? 'ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙØªØ§Ø­' : 'Ø§Ù„Ù…ÙØªØ§Ø­ ÙØ§Ø¶ÙŠ!')
    );
    const response = await axios.post(
      'https://api.lemonfox.ai/v1/audio/transcriptions',
      body,
      {
        headers: {
          Authorization: `Bearer ${process.env.LEMONFOX_API_KEY}`,
          ...body.getHeaders(),
        },
      }
    );
    console.log('âœ… Audio transcribed with LemonFox:', response.data.text);
    return response.data.text;
  } catch (err) {
    console.error('âŒ Error transcribing audio with LemonFox:', err.message, err.stack);
    throw new Error(`Failed to transcribe audio: ${err.message}`);
  }
}

async function processMessage(botId, userId, message, isImage = false, isVoice = false, history = []) {
  try {
    console.log('ðŸ¤– Processing message for bot:', botId, 'user:', userId, 'message:', message);

    const rules = await Rule.find({ $or: [{ botId }, { type: 'global' }] });
    console.log('ðŸ“œ Rules found:', rules);

    let systemPrompt = 'Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ©:\n';
    if (rules.length === 0) {
      systemPrompt += 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù‚Ù… Ø¨Ø§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙˆÙ…ÙÙŠØ¯.\n';
    } else {
      rules.forEach((rule) => {
        if (rule.type === 'global' || rule.type === 'general') {
          systemPrompt += `${rule.content}\n`;
        } else if (rule.type === 'products') {
          systemPrompt += `Ø§Ù„Ù…Ù†ØªØ¬: ${rule.content.product}ØŒ Ø§Ù„Ø³Ø¹Ø±: ${rule.content.price} ${rule.content.currency}\n`;
        } else if (rule.type === 'qa') {
          systemPrompt += `Ø§Ù„Ø³Ø¤Ø§Ù„: ${rule.content.question}ØŒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: ${rule.content.answer}\n`;
        }
      });
    }
    console.log('ðŸ“ System prompt:', systemPrompt);

    let conversation = await Conversation.findOne({ botId, userId });
    if (!conversation) {
      console.log('ðŸ“‹ Creating new conversation for bot:', botId, 'user:', userId);
      conversation = await Conversation.create({ botId, userId, messages: [] });
    } else {
      console.log('ðŸ“‹ Found existing conversation:', conversation._id);
    }

    let userMessageContent = message;

    if (isVoice) {
      userMessageContent = await transcribeAudio(message);
      if (!userMessageContent) {
        throw new Error('Failed to transcribe audio: No text returned');
      }
      console.log('ðŸ’¬ Transcribed audio message:', userMessageContent);
    }

    conversation.messages.push({ role: 'user', content: userMessageContent, timestamp: new Date() });
    await conversation.save();
    console.log('ðŸ’¬ User message added to conversation:', userMessageContent);

    const lastUserMessage = conversation.messages
      .filter(msg => msg.role === 'user')
      .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))[0];
    const lastMessageTimestamp = lastUserMessage ? new Date(lastUserMessage.timestamp) : new Date();

    // Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù…
    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.map(msg => ({ role: msg.role, content: msg.content })), // Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    ];

    if (isImage) {
      messages.push({
        role: 'user',
        content: [
          { type: 'text', text: 'Analyze this image:' },
          { type: 'image_url', image_url: { url: message } },
        ],
      });
      console.log('ðŸ–¼ï¸ Processing image message');
    }

    console.log('ðŸ“¡ Calling OpenAI API...');
    let reply = '';

    if (message.includes('Ø§Ù†ØªÙˆØ§ ÙØ§ØªØ­ÙŠÙ†') || message.includes('Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„')) {
      const timePrompt = `
        Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ. Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø¯ ØªØªØ¶Ù…Ù† Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„:
        ${systemPrompt}
        Ø§Ù„Ø³Ø¤Ø§Ù„: "${message}"
        Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ (Ù…Ø«Ù„ "Ù…Ù† 9 Ø§Ù„ØµØ¨Ø­ Ù„Ù€ 5 Ø§Ù„Ù…ØºØ±Ø¨" Ø£Ùˆ "Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© 8 Ù„Ù„Ø³Ø§Ø¹Ø© 6") Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯.
        Ù„Ùˆ Ù„Ù‚ÙŠØª Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¹Ù…Ù„ØŒ Ø±Ø¬Ø¹Ù‡Ø§ ÙÙŠ ØµÙŠØºØ©: "Ù…Ù† HH:MM Ø¥Ù„Ù‰ HH:MM" (Ø¨ØªÙˆÙ‚ÙŠØª 24 Ø³Ø§Ø¹Ø©).
        Ù„Ùˆ Ù…Ø§ Ù„Ù‚ÙŠØªØ´ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¹Ù…Ù„ØŒ Ø±Ø¬Ø¹: "Ù„Ù… Ø£Ø¬Ø¯ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯."
      `;

      const timeResponse = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'system', content: timePrompt }],
        max_tokens: 100,
      });

      let workingHoursText = timeResponse.choices[0].message.content;
      let workingHours = { start: '09:00', end: '17:00' };

      if (workingHoursText !== 'Ù„Ù… Ø£Ø¬Ø¯ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯.') {
        const match = workingHoursText.match(/Ù…Ù† (\d{2}:\d{2}) Ø¥Ù„Ù‰ (\d{2}:\d{2})/);
        if (match) {
          workingHours.start = match[1];
          workingHours.end = match[2];
        }
      } else {
        reply = 'Ù„Ù… Ø£Ø¬Ø¯ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯. ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØªÙ‡Ø§ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙŠ!';
      }

      if (!reply) {
        const now = lastMessageTimestamp;
        const startTime = new Date(now.toDateString() + ' ' + workingHours.start);
        const endTime = new Date(now.toDateString() + ' ' + workingHours.end);
        const isOpen = now >= startTime && now <= endTime;

        reply = `Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ Ù…Ù† ${workingHours.start} Ø¥Ù„Ù‰ ${workingHours.end}. Ø§Ù„ÙˆÙ‚Øª Ø¯Ù„ÙˆÙ‚ØªÙŠ ${lastMessageTimestamp.toLocaleString('ar-EG')}. ${isOpen ? 'Ø¥Ø­Ù†Ø§ ÙØ§ØªØ­ÙŠÙ† Ø¯Ù„ÙˆÙ‚ØªÙŠ!' : 'Ù„Ù„Ø£Ø³Ù Ø¥Ø­Ù†Ø§ Ù…ØºÙ„Ù‚ÙŠÙ† Ø¯Ù„ÙˆÙ‚ØªÙŠ.'}`;
      }
    } else {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages,
        max_tokens: 700,
      });
      reply = response.choices[0].message.content;
    }

    if (reply.includes('${lastMessageTimestamp.toLocaleString(\'ar-EG\')}')) {
      reply = reply.replace('${lastMessageTimestamp.toLocaleString(\'ar-EG\')}', lastMessageTimestamp.toLocaleString('ar-EG'));
    }
    if (reply.includes('${lastMessageTimestamp.toISOString()}')) {
      reply = reply.replace('${lastMessageTimestamp.toISOString()}', lastMessageTimestamp.toISOString());
    }

    conversation.messages.push({ role: 'assistant', content: reply, timestamp: new Date() });
    await conversation.save();
    console.log('ðŸ’¬ Assistant reply added to conversation:', reply);

    return reply;
  } catch (err) {
    console.error('âŒ Error processing message:', err.message, err.stack);
    return 'Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ.';
  }
}

module.exports = { processMessage };
