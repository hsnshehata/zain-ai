// /server/botEngine.js

const OpenAI = require('openai');
const mongoose = require('mongoose');
const axios = require('axios');
const FormData = require('form-data');
const Bot = require('./models/Bot');

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const conversationSchema = new mongoose.Schema({
  botId: { type: mongoose.Schema.Types.ObjectId, ref: 'Bot', required: true },
  userId: { type: String, required: true },
  messages: [
    {
      role: { type: String, enum: ['user', 'assistant'], required: true },
      content: { type: String, required: true },
      timestamp: { type: Date, default: Date.now },
    },
  ],
});

const Conversation = mongoose.model('Conversation', conversationSchema);

const Rule = require('./models/Rule');

// Ø¯Ø§Ù„Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
function getCurrentTime() {
  return new Date().toLocaleString('ar-EG', { timeZone: 'Africa/Cairo' });
}

async function transcribeAudio(audioUrl) {
  const body = new FormData();
  body.append('file', audioUrl);
  body.append('language', 'arabic');
  body.append('response_format', 'json');
  try {
    console.log(
      'LemonFox API Key: ' +
        (process.env.LEMONFOX_API_KEY ? 'ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙØªØ§Ø­' : 'Ø§Ù„Ù…ÙØªØ§Ø­ ÙØ§Ø¶ÙŠ!')
    );
    const response = await axios.post(
      'https://api.lemonfox.ai/v1/audio/transcriptions',
      body,
      {
        headers: {
          Authorization: `Bearer ${process.env.LEMONFOX_API_KEY}`,
          ...body.getHeaders(),
        },
      }
    );
    console.log('âœ… Audio transcribed with LemonFox:', response.data.text);
    return response.data.text;
  } catch (err) {
    console.error('âŒ Error transcribing audio with LemonFox:', err.message, err.stack);
    throw new Error(`Failed to transcribe audio: ${err.message}`);
  }
}

async function processMessage(botId, userId, message, isImage = false, isVoice = false) {
  try {
    console.log('ğŸ¤– Processing message for bot:', botId, 'user:', userId, 'message:', message);

    const rules = await Rule.find({ $or: [{ botId }, { type: 'global' }] });
    console.log('ğŸ“œ Rules found:', rules);

    // Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ systemPrompt Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
    let systemPrompt = `Ø£Ù†Øª Ø¨ÙˆØª Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ©. Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ù‡Ùˆ: ${getCurrentTime()}.\n`;
    if (rules.length === 0) {
      systemPrompt += 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù‚Ù… Ø¨Ø§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙˆÙ…ÙÙŠØ¯.\n';
    } else {
      rules.forEach((rule) => {
        if (rule.type === 'global' || rule.type === 'general') {
          systemPrompt += `${rule.content}\n`;
        } else if (rule.type === 'products') {
          systemPrompt += `Ø§Ù„Ù…Ù†ØªØ¬: ${rule.content.product}ØŒ Ø§Ù„Ø³Ø¹Ø±: ${rule.content.price} ${rule.content.currency}\n`;
        } else if (rule.type === 'qa') {
          systemPrompt += `Ø§Ù„Ø³Ø¤Ø§Ù„: ${rule.content.question}ØŒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: ${rule.content.answer}\n`;
        }
      });
    }
    console.log('ğŸ“ System prompt:', systemPrompt);

    let conversation = await Conversation.findOne({ botId, userId });
    if (!conversation) {
      console.log('ğŸ“‹ Creating new conversation for bot:', botId, 'user:', userId);
      conversation = await Conversation.create({ botId, userId, messages: [] });
    } else {
      console.log('ğŸ“‹ Found existing conversation:', conversation._id);
    }

    let userMessageContent = message;

    if (isVoice) {
      userMessageContent = await transcribeAudio(message);
      if (!userMessageContent) {
        throw new Error('Failed to transcribe audio: No text returned');
      }
      console.log('ğŸ’¬ Transcribed audio message:', userMessageContent);
    }

    conversation.messages.push({ role: 'user', content: userMessageContent, timestamp: new Date() });
    await conversation.save();
    console.log('ğŸ’¬ User message added to conversation:', userMessageContent);

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversation.messages.map((msg) => ({ role: msg.role, content: msg.content })),
    ];

    if (isImage) {
      messages.push({
        role: 'user',
        content: [
          { type: 'text', text: 'Analyze this image:' },
          { type: 'image_url', image_url: { url: message } },
        ],
      });
      console.log('ğŸ–¼ï¸ Processing image message');
    }

    console.log('ğŸ“¡ Calling OpenAI API...');
    let reply = '';

    // Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚Ø¨Ù„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI
    for (const rule of rules) {
      if (rule.type === 'qa' && userMessageContent.toLowerCase().includes(rule.content.question.toLowerCase())) {
        reply = rule.content.answer;
        break;
      } else if (rule.type === 'general' || rule.type === 'global') {
        if (userMessageContent.toLowerCase().includes(rule.content.toLowerCase())) {
          reply = rule.content;
          break;
        }
      } else if (rule.type === 'products') {
        if (userMessageContent.toLowerCase().includes(rule.content.product.toLowerCase())) {
          reply = `Ø§Ù„Ù…Ù†ØªØ¬: ${rule.content.product}ØŒ Ø§Ù„Ø³Ø¹Ø±: ${rule.content.price} ${rule.content.currency}`;
          break;
        }
      }
    }

    // Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø©ØŒ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI
    if (!reply) {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages,
        max_tokens: 700,
      });
      reply = response.choices[0].message.content;
    }

    conversation.messages.push({ role: 'assistant', content: reply, timestamp: new Date() });
    await conversation.save();
    console.log('ğŸ’¬ Assistant reply added to conversation:', reply);

    return reply;
  } catch (err) {
    console.error('âŒ Error processing message:', err.message, err.stack);
    return 'Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ.';
  }
}

module.exports = { processMessage };
